//
//  SanderWoodTokenizer.swift
//  ondevicetext2music
//
//  Created by Max Fr on 3/6/25.
//

import Foundation
import CoreML

public class SanderWoodTokenizer {
    struct TokenizerConfig{
        let eosTokenId: Int32
        let padTokenId: Int32
        let decoderStartTokenId: Int32
    }
    
    private var tokenizer: SimpleVocabTokenizer?
    private var tokenizerConfig: TokenizerConfig!
    
    // MARK: Init
    public init() {
        do{
            guard let tokenizerDirURL = Bundle.main.url(forResource: "model_config", withExtension: nil)
            else {
                throw NSError(domain: "TokenizerURLError", code: 404, userInfo: nil)
            }
            
            let vocabPath = tokenizerDirURL.appendingPathComponent("vocab.json")
            
            self.tokenizer = try SimpleVocabTokenizer(vocabPath: vocabPath)
            print("âœ… Successfully loaded tokenzier")
            
            // config config
            self.tokenizerConfig = TokenizerConfig(
                eosTokenId: 2,   // </s> token ID
                padTokenId: 1,   // <pad> token ID
                decoderStartTokenId: 2  // ä»0ä¿®æ”¹ä¸º2ï¼Œä¸åŸæ¨¡å‹é…ç½®ä¸­çš„decoder_start_token_idåŒ¹é…
            )
        }
        catch{
            print("âŒ Failed to initialize tokenizer: \(error)")
        }
    }
    
    // MARK: - tokenize
    public func tokenize(text: String) throws -> (MLMultiArray, MLMultiArray) {
        print("ğŸ”¤ SanderWoodTokenizer: å¼€å§‹tokenizeæ–‡æœ¬: \(text)")
        
        guard let tokenizer = tokenizer else {
            print("ğŸ”¤ SanderWoodTokenizer: âŒ é”™è¯¯: tokenizeræœªåˆå§‹åŒ–")
            throw MLServiceError.tokenizerNotInitialized
        }
        
        // ä½¿ç”¨æ–°çš„tokenizerç¼–ç 
        print("ğŸ”¤ SanderWoodTokenizer: ä½¿ç”¨SimpleVocabTokenizerç¼–ç ")
        let tokens = tokenizer.encode(text)
        print("ğŸ”¤ SanderWoodTokenizer: ç¼–ç å®Œæˆï¼Œå¾—åˆ°\(tokens.count)ä¸ªtoken")
        
        // padding token (0)
        // tokenå‡ºæ¥çš„è‡ªå·±å°±å¸¦ä¸€ä¸ª(1)
        let tokensArray = tokens.map { NSNumber(value: $0) }
        let length = tokens.count
        
        // ç¡®ä¿é•¿åº¦ä¸ä¸º0
        if length == 0 {
            print("ğŸ”¤ SanderWoodTokenizer: âš ï¸ è­¦å‘Š: tokené•¿åº¦ä¸º0")
        }
        
        print("ğŸ”¤ SanderWoodTokenizer: åˆ›å»ºMLMultiArrayï¼Œå½¢çŠ¶: [1, \(length)]")
        
        // ä½¿ç”¨å®‰å…¨çš„æ–¹å¼åˆ›å»ºMLMultiArray
        let inputIds: MLMultiArray
        let attentionMask: MLMultiArray
        
        do {
            // å°è¯•åˆ›å»ºinput_ids
            inputIds = try MLMultiArray(shape: [1, NSNumber(value: length)], dataType: .int32)
            print("ğŸ”¤ SanderWoodTokenizer: æˆåŠŸåˆ›å»ºinputIds MLMultiArray")
            // å°è¯•åˆ›å»ºattention_mask
            attentionMask = try MLMultiArray(shape: [1, NSNumber(value: length)], dataType: .int32)
            print("ğŸ”¤ SanderWoodTokenizer: æˆåŠŸåˆ›å»ºattentionMask MLMultiArray")
        } catch {
            print("ğŸ”¤ SanderWoodTokenizer: âŒ åˆ›å»ºMLMultiArrayå¤±è´¥: \(error)")
            throw error
        }
        
        // å¡«å…… input_ids å’Œ attention_mask
        print("ğŸ”¤ SanderWoodTokenizer: å¡«å……MLMultiArray")
        for i in 0..<length {
            do {
                let index = i
                inputIds[index] = tokensArray[i]
                // åªæœ‰ padding token (0) å¯¹åº”çš„ attention mask ä¸º 0
                attentionMask[index] = tokens[i] == 0 ? 0 : 1
            } catch {
                print("ğŸ”¤ SanderWoodTokenizer: âŒ è®¿é—®MLMultiArrayç´¢å¼•\(i)å¤±è´¥: \(error)")
                throw error
            }
        }
        // å¼ºåˆ¶è®¾ç½®æˆ1
        attentionMask[0] = 1
        print(inputIds)
        print(attentionMask)
        print("ğŸ”¤ SanderWoodTokenizer: æˆåŠŸå¡«å……MLMultiArray")
        return (inputIds, attentionMask)
    }
    
    // MARK: - decode
    public func decode(tokens: [Int32]) throws -> String {
        guard let tokenizer = tokenizer else {
            throw MLServiceError.tokenizerNotInitialized
        }
        
        return tokenizer.decode(tokens: tokens.map { Int($0) })
    }
}
